{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %pip install happytransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the required packages for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import happytransformer\n",
    "from happytransformer import HappyTextToText\n",
    "from happytransformer import TTEvalArgs\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import random\n",
    "import csv\n",
    "\n",
    "def random_extract(file_name, amount):\n",
    "    k=int(amount/175)+1\n",
    "    dct=0\n",
    "    with open(file_name,'w',newline='',encoding=\"utf-8\") as csvfile:\n",
    "        writter = csv.writer(csvfile)\n",
    "        writter.writerow([\"input\", \"target\"])\n",
    "        for i in range(1,176):\n",
    "            filename=\"Data/data_\"+str(i)+\".csv\"\n",
    "            n = sum(1 for line in open(filename,newline=\"\",encoding=\"utf-8\")) - 1\n",
    "            s = k\n",
    "            skip = sorted(random.sample(range(1,n+1),n-s))\n",
    "            df = pandas.read_csv(filename, skiprows=skip)\n",
    "            if (dct>=amount):\n",
    "                continue\n",
    "            for j in range (0,k):\n",
    "                writter.writerow([df['input'][j], df['target'][j]])\n",
    "                dct+=1\n",
    "                if (dct>=amount):\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the train, eval, test dataset can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_extract(\"train.csv\",3500)\n",
    "random_extract(\"eval.csv\",1500)\n",
    "random_extract(\"test.csv\",4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various models to choose from for pre-trained model, check the link for more detail:\n",
    "\n",
    "https://happytransformer.com/text-to-text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "06/09/2024 21:48:53 - INFO - happytransformer.happy_transformer -   Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "happy_tt = HappyTextToText(\"T5\", \"t5-base\")\n",
    "# before_result = happy_tt.eval(\"eval.csv\")\n",
    "# print(\"Before loss:\", before_result.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fine-tune part of the model, the path to save the model can be modified. Details for the parameters is in the following link:\n",
    "\n",
    "https://happytransformer.com/text-to-text/finetuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/09/2024 20:22:42 - INFO - happytransformer.happy_transformer -   Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0708f93b982443bbafc312d076579336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3100ac7232040f9b486496cc4c59323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f848f36ddc48619d51eef6c3849187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9207, 'grad_norm': 5.709052085876465, 'learning_rate': 4.9873096446700515e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1285711850e4a13856116adeda5e190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1308772563934326, 'eval_runtime': 72.1058, 'eval_samples_per_second': 4.854, 'eval_steps_per_second': 0.61, 'epoch': 0.0}\n",
      "{'loss': 0.5289, 'grad_norm': 2.869354724884033, 'learning_rate': 4.492385786802031e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a410b4e1e52430197d243bb19e79fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23966260254383087, 'eval_runtime': 17.7755, 'eval_samples_per_second': 19.69, 'eval_steps_per_second': 2.475, 'epoch': 0.1}\n",
      "{'loss': 0.2152, 'grad_norm': 1.0123258829116821, 'learning_rate': 3.9847715736040605e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae4a39b157e4250a35553833acb112a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1776588410139084, 'eval_runtime': 3.6905, 'eval_samples_per_second': 94.839, 'eval_steps_per_second': 11.923, 'epoch': 0.2}\n",
      "{'loss': 0.1955, 'grad_norm': 1.1548540592193604, 'learning_rate': 3.477157360406091e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab27cee9b183436ebc0a469125da73b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15514512360095978, 'eval_runtime': 3.8606, 'eval_samples_per_second': 90.66, 'eval_steps_per_second': 11.397, 'epoch': 0.3}\n",
      "{'loss': 0.1724, 'grad_norm': 1.8411650657653809, 'learning_rate': 2.969543147208122e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c3ae4394014996af6fb747aaa16f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14379647374153137, 'eval_runtime': 3.7316, 'eval_samples_per_second': 93.794, 'eval_steps_per_second': 11.791, 'epoch': 0.41}\n",
      "{'loss': 0.1757, 'grad_norm': 1.3429425954818726, 'learning_rate': 2.4619289340101523e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050a403d75e146538fd989e24a8f11f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13372297585010529, 'eval_runtime': 19.2256, 'eval_samples_per_second': 18.205, 'eval_steps_per_second': 2.289, 'epoch': 0.51}\n",
      "{'loss': 0.1602, 'grad_norm': 1.0087710618972778, 'learning_rate': 1.9543147208121827e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21a5312d0f849bd83510e7271923d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13203948736190796, 'eval_runtime': 5.995, 'eval_samples_per_second': 58.382, 'eval_steps_per_second': 7.339, 'epoch': 0.61}\n",
      "{'loss': 0.1535, 'grad_norm': 0.9898172616958618, 'learning_rate': 1.4467005076142132e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26334e721bad40faa89c7c9baaf43460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12902827560901642, 'eval_runtime': 7.4814, 'eval_samples_per_second': 46.782, 'eval_steps_per_second': 5.881, 'epoch': 0.71}\n",
      "{'loss': 0.1484, 'grad_norm': 2.514209508895874, 'learning_rate': 9.390862944162437e-06, 'epoch': 0.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39938658d8dc44c09a3f4a337b9db3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12798011302947998, 'eval_runtime': 21.3619, 'eval_samples_per_second': 16.384, 'eval_steps_per_second': 2.06, 'epoch': 0.81}\n",
      "{'loss': 0.1646, 'grad_norm': 1.1851485967636108, 'learning_rate': 4.3147208121827415e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff5992225d84ba0a6ed3b45d05c51fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12459461390972137, 'eval_runtime': 8.33, 'eval_samples_per_second': 42.017, 'eval_steps_per_second': 5.282, 'epoch': 0.91}\n",
      "{'train_runtime': 554.5528, 'train_samples_per_second': 5.68, 'train_steps_per_second': 0.71, 'train_loss': 0.20635919477128742, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import TTTrainArgs\n",
    "args = TTTrainArgs(batch_size=8, max_input_length=100, max_output_length=100)\n",
    "happy_tt.train(\"train.csv\", args=args)\n",
    "happy_tt.save(\"model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/28/2022 04:27:21 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.06it/s]\n",
      " 90%|█████████ | 9/10 [00:01<00:00,  5.77ba/s]\n",
      "PyTorch: setting up devices\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 1\n",
      "100%|██████████| 10000/10000 [30:58<00:00,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loss:  0.08027171343564987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "after_loss = happy_tt.eval(\"eval.csv\")\n",
    "print(\"After loss: \", after_loss.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the settings for obtaining results of a given input from a fine-tuned model, details for the parameter are in the following link:\n",
    "\n",
    "https://happytransformer.com/text-to-text/settings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from happytransformer import TTSettings\n",
    "beam_settings =  TTSettings(num_beams=1, min_length=10, max_length=1000)\n",
    "greedy_settings = TTSettings(no_repeat_ngram_size=2, max_length=20)\n",
    "top_k_sampling_settings = TTSettings(do_sample=True, top_k=50, temperature=0.7, max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models stored in a given path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/09/2024 23:38:06 - WARNING - happytransformer.happy_transformer -   load_path has been deprecated. Provide the load_path to the  model_name parameter instead t5-small. load_path will be removed in a later version. For now, we'll load the model form the load_path provided.  \n",
      "06/09/2024 23:38:07 - INFO - happytransformer.happy_transformer -   Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model=HappyTextToText(load_path=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate results from the test dataset using the model above, the path to store the result can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/09/2024 23:38:13 - INFO - happytransformer.happy_transformer -   Moving model to cuda:0\n",
      "06/09/2024 23:38:14 - INFO - happytransformer.happy_transformer -   Initializing a pipeline\n"
     ]
    }
   ],
   "source": [
    "def runtest(model, path, save):\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as csvfile, open(save, 'w', newline=\"\", encoding=\"utf-8\") as csvf:\n",
    "        data=csv.reader(csvfile)\n",
    "        writter = csv.writer(csvf) \n",
    "        writter.writerow([\"input\",\"target\"])\n",
    "        k=0\n",
    "        for row in data:\n",
    "            if (k==0):\n",
    "                k+=1\n",
    "                continue\n",
    "            result=model.generate_text(row[0], args=beam_settings).text\n",
    "            writter.writerow([result])\n",
    "\n",
    "runtest(model,'test.csv', 'model.csv') #model, {test_data_path}, {store_data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1_score calculation of the result above and the gruond truth, result of each punctuation is separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "#[. , ! ? - : ;]\n",
    "#[TP,TN,FP,FN]\n",
    "#[acc,pre,rec]\n",
    "def f_1(path, result):\n",
    "    total=0\n",
    "    ct=0\n",
    "    with open(path, 'r', newline=\"\", encoding=\"utf-8\") as csv1, open(result, 'r', newline=\"\", encoding=\"utf-8\") as csv2:\n",
    "        f_1T=[[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "        pun=\".,!?-:;\"\n",
    "        csv_r1=pd.read_csv(csv1)\n",
    "        csv_r2=pd.read_csv(csv2)      \n",
    "        res=[]\n",
    "        truth=[]\n",
    "        k=0\n",
    "        for sentence in csv_r1.iterrows():\n",
    "            truth.append(sentence[1]['target'][1:-1])\n",
    "        for sentence in csv_r2.iterrows():\n",
    "            res.append(sentence[1]['input'][1:-1])\n",
    "\n",
    "        l=len(truth)\n",
    "    \n",
    "        for i in range(l):\n",
    "            dt=truth[i].split(\" \")\n",
    "            r=res[i].split(\" \")\n",
    "            list_1=[[],[],[],[],[],[],[],0]\n",
    "            list_2=[[],[],[],[],[],[],[],0]\n",
    "            for i in dt:\n",
    "                if i not in pun or i==\"\\\"\":\n",
    "                    list_1[7]+=1\n",
    "                else:\n",
    "                    list_1[int((pun.find(i))/2)].append(list_1[7])\n",
    "            for i in r:\n",
    "                if i not in pun or i==\"\\\"\":\n",
    "                    list_2[7]+=1\n",
    "                else:\n",
    "                    list_2[int((pun.find(i))/2)].append(list_2[7])\n",
    "            total+=1\n",
    "            s=max(list_1[7],list_2[7])\n",
    "            for i in range(7):\n",
    "                for j in range (1,s+1):\n",
    "                    if j in list_1[i] and j in list_2[i]:\n",
    "                        f_1T[i][0]+=1\n",
    "                    if (j not in list_1[i] and j not in list_2[i]):\n",
    "                        f_1T[i][1]+=1\n",
    "                    if (j not in list_1[i] and j in list_2[i]):\n",
    "                        f_1T[i][2]+=1\n",
    "                    if (j in list_1[i] and j not in list_2[i]):\n",
    "                        f_1T[i][3]+=1\n",
    "        f_1R=[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]\n",
    "        punc=[\"period\",\"comma\",\"exclamation mark\",\"question mark\",\"hyphen\",\"colon\",\"semicolon\"]\n",
    "\n",
    "        for i in range(7):\n",
    "            if (sum(f_1T[i])==0):\n",
    "                f_1R[i][0]=-1\n",
    "            else:\n",
    "                f_1R[i][0]=(f_1T[i][0]+f_1T[i][1])/(sum(f_1T[i]))\n",
    "    \n",
    "            if (f_1T[i][0]+f_1T[i][2]>0):\n",
    "                f_1R[i][1]=(f_1T[i][0])/(f_1T[i][0]+f_1T[i][2])\n",
    "            \n",
    "            if (f_1T[i][0]+f_1T[i][3]==0):\n",
    "                f_1R[i][2]=-1\n",
    "            else:\n",
    "                f_1R[i][2]=(f_1T[i][0])/(f_1T[i][0]+f_1T[i][3])\n",
    "            if (f_1R[i][1]+f_1R[i][2]==0):\n",
    "                f1=-1\n",
    "            else:\n",
    "                f1=2*f_1R[i][1]*f_1R[i][2]/(f_1R[i][1]+f_1R[i][2])\n",
    "        print(punc[i])\n",
    "        print(f\"acc: {f_1R[i][0]}, pre: {f_1R[i][1]}, rec: {f_1R[i][2]}, f1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semicolon\n",
      "acc: 1.0, pre: 0, rec: -1, f1: -0.0\n"
     ]
    }
   ],
   "source": [
    "f_1(\"test.csv\",\"model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string1 = \"Speaking from our Empire’s oldest capital city , war-battered but never for one moment daunted or dismayed – speaking from London , I ask you to join with me in that act of thanksgiving .\"\n",
    "test_string2 = \"Nature's bequest gives nothing but doth lend , And being frank she lends to those are free : Then beauteous niggard why dost thou abuse , The bounteous largess given thee to give ?\"\n",
    "test_string3 = \"Numpy is a great framework , but it cannot utilize GPUs to accelerate its numerical computations . For modern deep neural networks , GPUs often provide speedups of 50x or greater , so unfortunately numpy won’t be enough for modern deep learning .\"\n",
    "test_string4 = \"Phylogenetic analyses were carried out using ML as implemented in RAxML v8.0.0 (Stamatakis , 2014) through the CIPRES Science Gateway (Miller , Pfeiffer & Scwartz , 2010) and Bayesian methods using BEAST v1.7.5 (Drummond & Rambaut , 2007) , using the partitioning schemes determined by PartitionFinder .\"\n",
    "test_string5 = \"So he huffed and he puffed , and he blew his house in , and ate up the little Pig . The second Pig met a Man with a bundle of furze , and said , \\\" Please , Man , give me that furze to build a house \\\" ; which the Man did .\"\n",
    "test_string6 = \"Neither is technology a ‘super’ actor stifling all other interactions as presented in the strong form of technological determinism ; its agency is constituted by others and in turn constitutes the actions of others .\"\n",
    "test_strings=[test_string1,test_string2,test_string3,test_string4,test_string5,test_string6]\n",
    "\n",
    "model_string_1 = \"Speaking from our Empire's oldest capital city war battered but never for one moment daunted or dismayed speaking from London I ask you to join me in that act of thanksgiving .\"\n",
    "model_string_2 = \"Nature's bequest gives nothing but doth lend . And being frank , she lends to those are free . Then beauteous niggard , why dost thou abuse The bounteous largess given thee to give .\"\n",
    "model_string_3 = \"Numpy is a great framework but it cannot utilize GPUs to accelerate its numerical computations . For modern deep neural networks GPUs often provide speedups of 50x or greater so unfortunately numpy won't be enough for modern deep learning .\"\n",
    "model_string_4 = \"Phylogenetic analyses were carried out using ML as implemented in RAxML v8 0 0 Stamatakis 2014 through the CIPRES Science Gateway Miller Pfeiffer Scwartz 2010 . Bayesian methods using BEAST v1 7 5 Drummond Rambaut 2007 using the partitioning schemes determined by PartitionFinder .\"\"\"\n",
    "model_string_5 = \"So he huffed and he puffed and he blew his house in and ate up the little Pig The second Pig met a Man with a bundle of furze and said Please Man give me that furze to build a house which the Man did .\"\n",
    "model_string_6 = \"Neither is technology a super actor stifling all other interactions , as presented in the strong form of technological determinism , its agency is constituted by others , and in turn constitutes the actions of others .\"\n",
    "model_strings=[model_string_1,model_string_2,model_string_3,model_string_4,model_string_5,model_string_6]\n",
    "\n",
    "clean_GPT4o_1 = \"Speaking from our Empire's oldest capital city , war-battered but never for one moment daunted or dismayed , speaking from London , I ask you to join with me in that act of thanksgiving .\"\n",
    "clean_GPT4o_2 = \"Nature's bequest gives nothing but doth lend ; And being frank , she lends to those are free : Then beauteous niggard , why dost thou abuse The bounteous largess given thee to give ?\"\n",
    "clean_GPT4o_3 = \"Numpy is a great framework , but it cannot utilize GPUs to accelerate its numerical computations . For modern deep neural networks , GPUs often provide speedups of 50x or greater , so unfortunately numpy won’t be enough for modern deep learning .\"\n",
    "clean_GPT4o_4 = \"Phylogenetic analyses were carried out using ML as implemented in RAxML v8.0.0 (Stamatakis , 2014) through the CIPRES Science Gateway (Miller , Pfeiffer , & Schwartz , 2010) and Bayesian methods using BEAST v1.7.5 (Drummond & Rambaut , 2007) , using the partitioning schemes determined by PartitionFinder .\"\n",
    "clean_GPT4o_5 = \"So he huffed , and he puffed , and he blew his house in and ate up the little Pig . The second Pig met a Man with a bundle of furze and said , \\\"Please , Man , give me that furze to build a house , \\\" which the Man did .\"\n",
    "clean_GPT4o_6 = \"Neither is technology a super actor stifling all other interactions as presented in the strong form of technological determinism ; its agency is constituted by others and in turn constitutes the actions of others .\"\n",
    "clean_GPT40=[clean_GPT4o_1,clean_GPT4o_2,clean_GPT4o_3,clean_GPT4o_4,clean_GPT4o_5,clean_GPT4o_6]\n",
    "\n",
    "\n",
    "clean_GPT35_1 = \"Speaking from our Empire's oldest capital city , war-battered but never for one moment daunted or dismayed , speaking from London , I ask you to join with me in that act of thanksgiving .\"\n",
    "clean_GPT35_2 = \"Nature's bequest gives nothing, but doth lend , And being frank , she lends to those are free . Then , beauteous niggard , why dost thou abuse The bounteous largess given thee to give ?\"\n",
    "clean_GPT35_3 = \"Numpy is a great framework , but it cannot utilize GPUs to accelerate its numerical computations . For modern deep neural networks , GPUs often provide speedups of 50x or greater , so unfortunately , numpy won't be enough for modern deep learning .\"\n",
    "clean_GPT35_4 = \"Phylogenetic analyses were carried out using ML as implemented in RAxML v8.0.0 (Stamatakis , 2014) through the CIPRES Science Gateway (Miller, Pfeiffer , & Schwartz , 2010) , and Bayesian methods using BEAST v1.7.5 (Drummond & Rambaut , 2007) , using the partitioning schemes determined by PartitionFinder .\"\n",
    "clean_GPT35_5 = \"So he huffed and he puffed and he blew his house in and ate up the little Pig . The second Pig met a Man with a bundle of furze and said , \\\" Please , Man , give me that furze to build a house , \\\" which the Man did .\"\n",
    "clean_GPT35_6 = \"Neither is technology a super actor stifling all other interactions as presented in the strong form of technological determinism ; its agency is constituted by others and , in turn , constitutes the actions of others .\"\n",
    "clean_GPT35=[clean_GPT35_1,clean_GPT35_2,clean_GPT35_3,clean_GPT35_4,clean_GPT35_5,clean_GPT35_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_compare(truth, result):\n",
    "    total=0\n",
    "    ct=0\n",
    "    f_1T=[[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "    pun=\".,!?-:;\"   \n",
    "    for i in range(6):\n",
    "        dt=truth[i].split(\" \")\n",
    "        r=result[i].split(\" \")\n",
    "        list_1=[[],[],[],[],[],[],[],0]\n",
    "        list_2=[[],[],[],[],[],[],[],0]\n",
    "        for i in dt:\n",
    "            if i not in pun or i==\"\\\"\":\n",
    "                list_1[7]+=1\n",
    "            else:\n",
    "                list_1[int((pun.find(i))/2)].append(list_1[7])\n",
    "        for i in r:\n",
    "            if i not in pun or i==\"\\\"\":\n",
    "                list_2[7]+=1\n",
    "            else:\n",
    "                list_2[int((pun.find(i))/2)].append(list_2[7])\n",
    "        total+=1\n",
    "        s=max(list_1[7],list_2[7])\n",
    "        for i in range(7):\n",
    "            for j in range (1,s+1):\n",
    "                if j in list_1[i] and j in list_2[i]:\n",
    "                    f_1T[i][0]+=1\n",
    "                if (j not in list_1[i] and j not in list_2[i]):\n",
    "                    f_1T[i][1]+=1\n",
    "                if (j not in list_1[i] and j in list_2[i]):\n",
    "                    f_1T[i][2]+=1\n",
    "                if (j in list_1[i] and j not in list_2[i]):\n",
    "                    f_1T[i][3]+=1\n",
    "    f_1R=[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]]\n",
    "    punc=[\"period\",\"comma\",\"exclamation mark\",\"question mark\",\"hyphen\",\"colon\",\"semicolon\"]\n",
    "\n",
    "    for i in range(7):\n",
    "        if (sum(f_1T[i])==0):\n",
    "            f_1R[i][0]=-1\n",
    "        else:\n",
    "            f_1R[i][0]=(f_1T[i][0]+f_1T[i][1])/(sum(f_1T[i]))\n",
    "\n",
    "        if (f_1T[i][0]+f_1T[i][2]>0):\n",
    "            f_1R[i][1]=(f_1T[i][0])/(f_1T[i][0]+f_1T[i][2])\n",
    "        \n",
    "        if (f_1T[i][0]+f_1T[i][3]==0):\n",
    "            f_1R[i][2]=-1\n",
    "        else:\n",
    "            f_1R[i][2]=(f_1T[i][0])/(f_1T[i][0]+f_1T[i][3])\n",
    "        \n",
    "        if (f_1R[i][1]+f_1R[i][2]==0):\n",
    "            f1=-1\n",
    "        else:\n",
    "            f1=2*f_1R[i][1]*f_1R[i][2]/(f_1R[i][1]+f_1R[i][2])\n",
    "        print(punc[i])\n",
    "        print(f\"acc: {f_1R[i][0]}, pre: {f_1R[i][1]}, rec: {f_1R[i][2]}, f1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period\n",
      "acc: 0.8558558558558559, pre: 0.26666666666666666, rec: 0.16, f1: 0.2\n",
      "comma\n",
      "acc: 0.9954954954954955, pre: 0, rec: 0.0, f1: -1\n",
      "exclamation mark\n",
      "acc: 0.9954954954954955, pre: 0, rec: 0.0, f1: -1\n",
      "question mark\n",
      "acc: 0.990990990990991, pre: 0, rec: 0.0, f1: -1\n",
      "hyphen\n",
      "acc: 1.0, pre: 0, rec: -1, f1: -0.0\n",
      "colon\n",
      "acc: 1.0, pre: 0, rec: -1, f1: -0.0\n",
      "semicolon\n",
      "acc: 1.0, pre: 0, rec: -1, f1: -0.0\n"
     ]
    }
   ],
   "source": [
    "f1_compare(test_strings,model_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period\n",
      "acc: 0.918552036199095, pre: 0.6296296296296297, rec: 0.68, f1: 0.6538461538461539\n",
      "comma\n",
      "acc: 1.0, pre: 1.0, rec: 1.0, f1: 1.0\n",
      "exclamation mark\n",
      "acc: 1.0, pre: 1.0, rec: 1.0, f1: 1.0\n",
      "question mark\n",
      "acc: 0.9909502262443439, pre: 0.5, rec: 0.5, f1: 0.5\n",
      "hyphen\n",
      "acc: 1.0, pre: 0, rec: -1, f1: -0.0\n",
      "colon\n",
      "acc: 1.0, pre: 0, rec: -1, f1: -0.0\n",
      "semicolon\n",
      "acc: 1.0, pre: 0, rec: -1, f1: -0.0\n"
     ]
    }
   ],
   "source": [
    "f1_compare(test_strings,clean_GPT40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period\n",
      "acc: 0.9095022624434389, pre: 0.5806451612903226, rec: 0.72, f1: 0.6428571428571428\n",
      "comma\n",
      "acc: 1.0, pre: 1.0, rec: 1.0, f1: 1.0\n",
      "exclamation mark\n",
      "acc: 0.995475113122172, pre: 0, rec: 0.0, f1: -1\n",
      "question mark\n",
      "acc: 0.995475113122172, pre: 1.0, rec: 0.5, f1: 0.6666666666666666\n",
      "hyphen\n",
      "acc: 1.0, pre: 0, rec: -1, f1: -0.0\n",
      "colon\n",
      "acc: 1.0, pre: 0, rec: -1, f1: -0.0\n",
      "semicolon\n",
      "acc: 1.0, pre: 0, rec: -1, f1: -0.0\n"
     ]
    }
   ],
   "source": [
    "f1_compare(test_strings,clean_GPT35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
